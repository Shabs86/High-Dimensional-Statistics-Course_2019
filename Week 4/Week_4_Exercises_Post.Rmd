---
title: "HDS Exercise set 4"
author: "Shabbeer Hassan"
output:
  pdf_document: 
     latex_engine: xelatex
theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(29)
#devtools::install_github('yihui/tinytex')
options(tinytex.verbose = TRUE)
```

```{r, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)
```

<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 30px;
  color: DarkRed;
}
h1 { /* Header 1 */
  font-size: 22px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 20px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
h4 { /* Header 4 */
  font-size: 14px;
  font-family: "Times New Roman", Times, serif;
  color: Black;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>


### Problem 1 -- Solution


```{r echo=T}

# Libraries
library(glmnet)
library(caret)
library(plotmo)
library(car)
library(tidyverse)
library(ggplot2)
library(MASS)
library(data.table)
library(dplyr)

tr.ind <- read.csv("~/Dropbox/Important_Documents/Doctoral_Work/Courses/High Dimensional Stats/2019/Week 3/HDS_ex3.3_tr.txt", sep=" ")

# Train & Test dataset
data(Boston)
train <- Boston[tr.ind$X1,]
test <- Boston[-tr.ind$X1,]

# Predictor variables
x <- model.matrix(medv~., train)[,-1]
x.test <- model.matrix(medv~., test)[,-1]

# Outcome variable
y <- train$medv
y.test <- test$medv
```



(a)

```{r echo=T}

# Finding the best lambda using cross-validation
cv_glmnet <- cv.glmnet(x, y, alpha = 0, type.measure = "mse")

c(cv_glmnet$lambda.min, cv_glmnet$lambda.1se)
round(log(c(cv_glmnet$lambda.min, cv_glmnet$lambda.1se)), 2)

plot(cv_glmnet) # PLotting model

```

####  According to the cv-plot, we see that normal regression aka OLS would do fine here since @ it corresponds to lambda = 0 and from the ridge reg above we see that lambda.min = 0.6521009. As the lambda increases, model predictions do not become better since MSE also increases.

####  The lambda.min option refers to value of 位 at the lowest CV error. The error at this value of 位 is the average of the errors over the k folds and hence this estimate of the error is uncertain. The lambda.1se represents the value of 位 in the search that was simpler than the best model (lambda.min), but which has error within 1 standard error of the best model. 
#### In other words, using the value of lambda.1se as the selected value for 位 results in a model that is slightly simpler than the best model but which cannot be distinguished from the best model in terms of error given the uncertainty in the k-fold CV estimate of the error of the best model. 
#### Hence, if we use lambda.min, we might get the best model that may be too complex and slightly overfitted. BUt lambda.1se gives us the simplest model that has comparable error to the best model given the uncertainty. 



(b)

```{r echo=T}

plot_glmnet(cv_glmnet$glmnet.fit, label = T, xvar = "lambda") 
abline(v = log(cv_glmnet$lambda.min), lty = 2) 
abline(v = log(cv_glmnet$lambda.1se), lty = 2)

# Run lm
fit_lm <- lm(medv~., train)
summary(fit_lm)
vif(fit_lm)
```

#### In glmnet(), the ones with the greatest effects are as from the plot: "rm", "chas" and "nox". 
#### We see that age and indus have been shrinked to zero and the ones remaining are included in the model. In lm(), we see that lstat, dis, rm and ptratio are amongst the most highly signififcant variables. However, in glmnet() we see that "dis" is not to be seen at all despite having 5.56e-09 p-value. Also nox and chas are not among the highly significant variables in the linear regression but are seen to have greater effects in glmnet()


(c)

```{r echo=T}

Boston$chas<-as.numeric(Boston$chas)

#Standardize covariates before fitting
Boston.X.std<- scale(dplyr::select(Boston,-medv))
X.train<- as.matrix(Boston.X.std)[tr.ind$X1,]
X.test<-  as.matrix(Boston.X.std)[-tr.ind$X1,]
Y.train<- Boston[tr.ind$X1, "medv"]
Y.test<- Boston[-tr.ind$X1, "medv"]

cv_glmnet_std = cv.glmnet( x = X.train, y = Y.train, alpha = 0, type.measure = "mse")
plot_glmnet(cv_glmnet_std$glmnet.fit, label = T, xvar = "lambda") 
abline(v = log(cv_glmnet_std$lambda.min), lty = 2) 
abline(v = log(cv_glmnet_std$lambda.1se), lty = 2)
```


#### Interestingly, upon standardization the top three variables with the largest effects in glmnet() become rm, dis and lstat, which also are among the ones with lowest p-values in the linear regression. 


(d)

```{r echo=T}

#### Ridge Reg

# Cross Validation to find lamda.min
cv1<- cv.glmnet(x=x, y=y, family = "gaussian", alpha = 0, nfolds = 10, type.measure = "mse")

# Predictions
pred1.min<- predict(cv1, newx = x.test, s = "lambda.min")

# MSPE (prediction error)
cv1_mse <- mean((y.test-pred1.min)^2)
cv1_mse

# Deviance explained 
plot(cv1, xvar = "dev", label = TRUE)
cv1_var_expl <- 100-(cv1_mse*100 )/var(y )
cv1_var_expl

```

#### Variance explained is around 68%




### Problem 2 -- Solution

(e)

```{r echo=T}

###### LASSO 

library(coefplot)

# Cross Validation to find lamda.min
cv2<- cv.glmnet(x=x, y=y, family = "gaussian", alpha = 1, nfolds = 10,  type.measure = "mse")
cv2$lambda.min 
cv2$lambda.1se

# Predictions
pred2.1se<- predict(cv2, newx = x.test, s = "lambda.1se")

# MSPE (prediction error)
cv2_mse <- mean((y.test-pred2.1se)^2)
cv2_mse

# Deviance explained 
plot(cv2, xvar = "dev", label = TRUE)
cv2_var_expl <- 100-(cv2_mse*100 )/var(y )
cv2_var_expl
```

#### The MSE is smaller in ridge reg than lasso, which means that ridge explains teh test data better


(f). 

```{r echo=T}

# Run lm
fit_lm <- lm(medv~., train)
mse_lm <- mean((test$medv - predict.lm(fit_lm, test)) ^ 2)
mse_lm

```

#### LM has lowest mse, followed by ridge and then LASSO, which means linear model should be used here instead of penalised reg.


(g).

```{r echo=T}

# Using lamda.min (ridge)
ridge_fit <- glmnet( x, y, lambda = cv1$lambda.min, alpha = 0 ) 

# Using lamda.1se (LASSO)
lasso_fit <- glmnet( x, y, lambda = cv2$lambda.1se, alpha = 1 ) 

# Coefficient table
coef_tab <- data.frame(name = c('Intercept', as.vector(colnames(x))), 
                       lm = as.vector(coefficients(fit_lm)),  
                       ridge = as.vector(coef(ridge_fit)), 
                       lasso = as.vector(coef(lasso_fit)))
coef_tab

```

##### There are 5 such variables for whom the coefficients are set to 0 in lasso model - crim, indus, age, rad, tax

##### The selected lamda values chosen for the models were not so large that the penalization could be heavier, leading the coefficients towards zero. So, if not large lamda values, then perhaphs variables were correlated towards wach other (multicollinearity) which is tended to be ignored by lasso but worked on heavily by ridge reg. Thats why much of teh coeff in lasso are set to zero. 



### Problem 3 -- Solution

```{r echo=T}

new_dat <- read.csv("~/Dropbox/Important_Documents/Doctoral_Work/Courses/High Dimensional Stats/2019/Week 4/HDS_ex4_n300p240.txt", sep=" ")

# Train & Test dataset
train <- new_dat[which(new_dat$train=='1'), ]
test <- new_dat[which(new_dat$train=='0'), ]

#Standardize covariates before fitting
train_std <- scale(dplyr::select(train,-c(y1, y2, train)))
test_std <- scale(dplyr::select(test,-c(y1, y2, train)))

# Predictors only
x_pred_train <- train[, -c(1:3)]
x_pred_test <- test[, -c(1:3)]  

# Outcome vars
Y1.train<- train$y1
Y2.train<- train$y2
Y1.test<- test$y1
Y2.test<- test$y2
Y.test<- Boston[-tr.ind$X1, "medv"]

##### RIDGE ####

# Ridge reg using default lambda from cv for y1
cv.y1_ridge <- cv.glmnet(x=as.matrix(x_pred_train), y=Y1.train, family = "gaussian", alpha = 0, nfolds = 10,  type.measure = "mse") # find lambda
cv.y1_ridge$lambda.min # minimum lambda during CV
cv.y1_ridge$lambda.1se
plot_glmnet(cv.y1_ridge$glmnet.fit, label = T, xvar = "lambda") 
abline(v = log(cv.y1_ridge$lambda.min), lty = 2) 
abline(v = log(cv.y1_ridge$lambda.1se), lty = 2)

# Ridge reg after adjusting lambda for y1
cv.y1_ridge_adjust <- cv.glmnet(x=as.matrix(x_pred_train), y=Y1.train, family = "gaussian", alpha = 0, nfolds = 10,  type.measure = "mse", lambda = seq(0,10,0.01))
cv.y1_ridge_adjust$lambda.min # minimum lambda during CV
cv.y1_ridge_adjust$lambda.1se
plot(cv.y1_ridge_adjust$glmnet.fit, label = F, xvar = "lambda") 
abline(v = log(cv.y1_ridge_adjust$lambda.min), lty = 2) 
abline(v = log(cv.y1_ridge_adjust$lambda.1se), lty = 2)

# MSE
pred_ridge <- predict(cv.y1_ridge_adjust, newx = as.matrix(x_pred_test)) 
ridge.MSE.y1 <- mean((as.vector(Y1.test) - pred_ridge)^2)
ridge.MSE.y1


# Ridge reg using default lambda from cv for y2
cv.y2_ridge <- cv.glmnet(x=as.matrix(x_pred_train), y=Y1.train, family = "gaussian", alpha = 0, nfolds = 10,  type.measure = "mse") # find lambda
cv.y2_ridge$lambda.min # minimum lambda during CV
cv.y2_ridge$lambda.1se
plot_glmnet(cv.y2_ridge$glmnet.fit, label = T, xvar = "lambda") 
abline(v = log(cv.y2_ridge$lambda.min), lty = 2) 
abline(v = log(cv.y2_ridge$lambda.1se), lty = 2)

# Ridge reg after adjusting lambda for y2
cv.y2_ridge_adjust <- cv.glmnet(x=as.matrix(x_pred_train), y=Y1.train, family = "gaussian", alpha = 0, nfolds = 10,  type.measure = "mse", lambda = seq(0,10,0.01))
cv.y2_ridge_adjust$lambda.min # minimum lambda during CV
cv.y2_ridge_adjust$lambda.1se
plot(cv.y2_ridge_adjust$glmnet.fit, label = F, xvar = "lambda") 
abline(v = log(cv.y2_ridge_adjust$lambda.min), lty = 2) 
abline(v = log(cv.y2_ridge_adjust$lambda.1se), lty = 2)

# MSE
pred_ridge <- predict(cv.y2_ridge_adjust, newx = as.matrix(x_pred_test)) 
ridge.MSE.y2 <- mean((as.vector(Y1.test) - pred_ridge)^2)
ridge.MSE.y2


##### LASSO ####

# LASSO using default lambda from cv for y1
cv.y1_lasso <- cv.glmnet(x=as.matrix(x_pred_train), y=Y1.train, family = "gaussian", alpha = 1, nfolds = 10,  type.measure = "mse") # find lambda
cv.y1_lasso$lambda.min # minimum lambda during CV
cv.y1_lasso$lambda.1se
plot_glmnet(cv.y1_lasso$glmnet.fit, label = T, xvar = "lambda") 
abline(v = log(cv.y1_lasso$lambda.min), lty = 2) 
abline(v = log(cv.y1_lasso$lambda.1se), lty = 2)

# LASSO after adjusting lambda for y1
cv.y1_lasso_adjust <- cv.glmnet(x=as.matrix(x_pred_train), y=Y1.train, family = "gaussian", alpha = 1, nfolds = 10,  type.measure = "mse", lambda = seq(0,10,0.01))
cv.y1_lasso_adjust$lambda.min # minimum lambda during CV
cv.y1_lasso_adjust$lambda.1se
plot(cv.y1_lasso_adjust$glmnet.fit, label = F, xvar = "lambda") 
abline(v = log(cv.y1_lasso_adjust$lambda.min), lty = 2) 
abline(v = log(cv.y1_lasso_adjust$lambda.1se), lty = 2)

# MSE
pred_lasso <- predict(cv.y1_lasso_adjust, newx = as.matrix(x_pred_test)) 
lasso.MSE.y1 <- mean((as.vector(Y1.test) - pred_lasso)^2)
lasso.MSE.y1


# LASSO using default lambda from cv for y2
cv.y2_lasso <- cv.glmnet(x=as.matrix(x_pred_train), y=Y1.train, family = "gaussian", alpha = 1, nfolds = 10,  type.measure = "mse") # find lambda
cv.y2_lasso$lambda.min # minimum lambda during CV
cv.y2_lasso$lambda.1se
plot_glmnet(cv.y2_lasso$glmnet.fit, label = T, xvar = "lambda") 
abline(v = log(cv.y2_lasso$lambda.min), lty = 2) 
abline(v = log(cv.y2_lasso$lambda.1se), lty = 2)

# LASSO after adjusting lambda for y2
cv.y2_lasso_adjust <- cv.glmnet(x=as.matrix(x_pred_train), y=Y1.train, family = "gaussian", alpha = 1, nfolds = 10,  type.measure = "mse", lambda = seq(0,10,0.01))
cv.y2_lasso_adjust$lambda.min # minimum lambda during CV
cv.y2_lasso_adjust$lambda.1se
plot(cv.y2_lasso_adjust$glmnet.fit, label = F, xvar = "lambda") 
abline(v = log(cv.y2_lasso_adjust$lambda.min), lty = 2) 
abline(v = log(cv.y2_lasso_adjust$lambda.1se), lty = 2)

# MSE
pred_lasso <- predict(cv.y2_lasso_adjust, newx = as.matrix(x_pred_test)) 
lasso.MSE.y2 <- mean((as.vector(Y1.test) - pred_lasso)^2)
lasso.MSE.y2


### MSE from both methods
MSE_tot <- data.frame(method = c("ridge.y1", "ridge.y2", "lasso.y1", "lasso.y2"), MSE = c(ridge.MSE.y1, ridge.MSE.y2,  lasso.MSE.y1, lasso.MSE.y2 ) )
MSE_tot 
```


#### According to MSE table above, we see that lasso regression is preferred for y1 and ridge regression is preferred for y2




### Problem 4 -- Solution

(a)
```{r echo=T}

dat <- read.csv("~/Dropbox/Important_Documents/Doctoral_Work/Courses/High Dimensional Stats/2019/Week 4/HDS_ex4_n300p240.txt", sep=" ")


lm_y1 <- apply(as.matrix(x_pred_train), 2, function(x)summary(lm(Y1.train ~ x ) )$coeff[2,4])
lm_y2 <- apply(as.matrix(x_pred_train), 2, function(x)summary(lm(Y2.train ~ x ) )$coeff[2,4])

qqplot(-log10(lm_y1), -log10(lm_y2), pch = 4) 
abline(0,1)
```


#### We see from the qqplot that p-values for y1 are higher than the for y2. As the p-values for y2 are smaller which would then mean that more significant predictors for y2 are found than for y1. Hence, we should use ridge regression for y2 and lasso for y1.






















#### Junk COde
imp <- as.data.frame(varImp(cv_glmnet))
imp <- data.frame(overall = imp$Overall,
           names   = rownames(imp))
imp[order(imp$overall,decreasing = T),]

varImp <- function(object, lambda = NULL, ...) {

  ## skipping a few lines

  beta <- predict(object, s = lambda, type = "coef")
  if(is.list(beta)) {
    out <- do.call("cbind", lapply(beta, function(x) x[,1]))
    out <- as.data.frame(out)
  } else out <- data.frame(Overall = beta[,1])
  out <- abs(out[rownames(out) != "(Intercept)",,drop = FALSE])
  out
}

#  Compare the models and see which variables agree
var_step = names(fit_lm$coefficients)[-1]
var_lasso = colnames(train)[which(coef(fit, s = cv.lasso$lambda.min)!=0)-1]
intersect(var_step,var_lasso)