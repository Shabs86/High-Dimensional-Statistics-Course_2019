---
title: "HDS Exercise set 2"
author: "Shabbeer Hassan"
output:
  pdf_document: default
  html_document: default
theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(29)
```

<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 30px;
  color: DarkRed;
}
h1 { /* Header 1 */
  font-size: 22px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 20px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
h4 { /* Header 4 */
  font-size: 14px;
  font-family: "Times New Roman", Times, serif;
  color: Black;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>


### Problem 1 -- Solutions

(a)
```{r echo=T}
library(qvalue)
data(hedenfalk)


## Histogram of p-values
hist(hedenfalk$p, prob = T, breaks = 50, xlab = "P-Values")
curve(dunif(x, 0, 1), 0, 1, add = T, col = "red", lwd = 4)
```


####  Null p-values follow a Uniform(0,1) distribution, which would result in a p-value like the relatively flat at the right tail of the histogram. The proportion of null p-values also appear to be quite large in comparison to non-null values

(b)
```{r echo=T}
## Manual estimate 
pval<- hedenfalk$p
lambda <- 0.5 
pi0_man <- sum(pval > lambda)/(length(pval)*lambda)

## Using qvalue package
qval_obj <- qvalue(p = pval)
#summary(qval_obj)
pi0_pkg <- qval_obj$pi0

# Estimates
pi0_man
pi0_pkg
```

(c) 
```{r echo=T}
# how many discoveries when determined the significance threshold by 
# P-value, by Q-value, by BH adjusted P-value or by Bonferroni corrected P-value

pval <- hedenfalk$p

# Threshold seq
t <- seq(0.01, 0.99, 0.01)

# Alpha 
Alpha_func <- function(x) {sum(pval < x)}
Alpha.eval <- sapply(t, Alpha_func)

# BH
BH_func <- function(x) {sum(p.adjust(pval, method = "BH", n = length(pval)) < x)}
BH.eval <- sapply(t, BH_func)

# Bonferroni
Bonf_func <- function(x) {sum(p.adjust(pval, method = "bonferroni", n = length(pval)) < x)}
Bonf.eval <- sapply(t, Bonf_func)

# Q-value
Q.eval_func <- function(x) {sum(qvalue(pval)$qvalues < x)}
Q.eval <- sapply(t, Q.eval_func)

# Combined df
Eval.df <- as.data.frame(cbind(t, Alpha.eval, BH.eval, Bonf.eval, Q.eval))

# Plot of above eval methods
library("reshape")
library("ggplot2")
Eval.df$t <- as.character(Eval.df$t)
Eval.df.trans <- melt(Eval.df, id.vars = "t")
ggplot(Eval.df.trans, aes(x = t, y = log10(value), 
                          group = variable, color = variable)) + 
  geom_point() + 
  geom_line() + 
  xlab("t") +
  ylab("Log(sum of discoveries)") +
  #scale_x_discrete(breaks = seq(0.01, 0.99, by = 0.1), limits=c(0.01, 0.99)) +
  #theme with white background
  theme_bw() +
  #eliminates background, gridlines, and chart border
  theme(
    plot.background = element_blank()
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
  ) +
  #draws x and y axis line
  theme(axis.line = element_line(color = 'black')) 
```

#### Its interesting to note that a constant alpha is the most liberal method while as expected Bonferoni is the most strict one. The other thing to note here is that as the threshold increases the sum of discoveries also increase, though Qvalue becomes very liberal at those higher values

(d) 
```{r echo=T}

# Plotiing qval
pval <- hedenfalk$p
q <- qvalue(pval)
plot(q)

# False discoveries
# How many discoveries you would make if you allowed 20 false discoveries? 
fdr = 20/length(pval)
qval_fdr_1 <- qvalue(pval, fdr.level = fdr)
sum(qval_fdr_1$significant == TRUE)

# What about if you allowed 10% of false discoveries among all discoveries?
qval_fdr_2 <- qvalue(pval, fdr.level = 0.1)
sum(qval_fdr_2$significant == TRUE)
```


### Problem 2 -- Solutions
```{r echo=T}

# Generate 5000 p-values
m = 1000
pval <- c(rbeta(m, 1, 100), runif(p-m, 0, 1))

# BH 
BH <- p.adjust(pval, method = "BH", n = length(pval))

# Qvalue
Qval <- qvalue(pval)
pi0 <- Qval$pi0  # overall proportion of true null hypotheses

# Linear Reg
lm.fit <- lm(Qval$qvalues ~ BH)
lm.fit$coefficients
```


#### On comparison of slope to the pi0 estimate from qval we see that they are similar to each other. 


### Problem 3 -- Solutions

(a)
```{r echo=T}

# Generate 5000 p-values
m = 500
p = 5000
pval <- c(rbeta(m, b.1, b.0), runif(p-m, 0, 1))

b.1 = 1
b.0 = 1
pval <- c(rbeta(m, b.1, b.0), runif(p-m, 0, 1))
hist(pval, xlab = "pval", prob = TRUE)
lines(density(pval))


b.1 = 1
b.0 = 100
pval <- c(rbeta(m, b.1, b.0), runif(p-m, 0, 1))
hist(pval, xlab = "pval", prob = TRUE)
lines(density(pval))


b.1 = 1
b.0 = 500
pval <- c(rbeta(m, b.1, b.0), runif(p-m, 0, 1))
hist(pval, xlab = "pval", prob = TRUE)
lines(density(pval))
```


(b)
```{r echo=T}

# Generate R = 500 replicates
R = 500
qval_fun <- function(x) {qvalue(p = x)}

#####

b.1 = 1
b.0 = 1

# Generate replicated pvalues and apply qval function over the columns
pval_new <- replicate(R, c(rbeta(m, b.1, b.0), runif(p-m, 0, 1)))
qval <- lapply(1:ncol(pval_new), function(x) qvalue(pval[,x]))

# Extract variables/elements from the above list
qval_rep <- lapply(qval, `[`, c('qvalues'))
qval_rep_df <- data.frame(matrix(unlist(qval_rep), nrow=length(qval_rep), byrow=T))

pi0_rep <- lapply(qval, `[`, c('pi0'))
pi0_rep_df <- data.frame(matrix(unlist(pi0_rep), nrow=length(pi0_rep), byrow=T))
names(pi0_rep_df) <- names(pi0_rep[[which(lengths(pi0_rep)>0)[1]]])

# Discoveries, FD, pi0
alpha=0.1
D = (sum(req_var_df$qval_rep_df < alpha))

D_func <- function(x) {(sum(qval_rep_df[,x] < alpha))}
Q.eval <- sapply(1:ncol(pval_new), D_func)


####

b.1 = 1
b.0 = 100

# Generate replicated pvalues and apply qval function over the columns
pval_new <- replicate(R, c(rbeta(m, b.1, b.0), runif(p-m, 0, 1)))
qval <- lapply(1:ncol(pval_new), function(x) qvalue(pval[,x]))

# Extract variables/elements from the above list
qval_rep <- lapply(qval, `[`, c('qvalues'))
qval_rep_df <- data.frame(matrix(unlist(qval_rep), nrow=length(qval_rep), byrow=T))

pi0_rep <- lapply(qval, `[`, c('pi0'))
pi0_rep_df <- data.frame(matrix(unlist(pi0_rep), nrow=length(pi0_rep), byrow=T))
names(pi0_rep_df) <- names(pi0_rep[[which(lengths(pi0_rep)>0)[1]]])


####

b.1 = 1
b.0 = 500
# Generate replicated pvalues and apply qval function over the columns
pval_new <- replicate(R, c(rbeta(m, b.1, b.0), runif(p-m, 0, 1)))
qval <- lapply(1:ncol(pval_new), function(x) qvalue(pval[,x]))

# Extract variables/elements from the above list
qval_rep <- lapply(qval, `[`, c('qvalues'))
qval_rep_df <- data.frame(matrix(unlist(qval_rep), nrow=length(qval_rep), byrow=T))

pi0_rep <- lapply(qval, `[`, c('pi0'))
pi0_rep_df <- data.frame(matrix(unlist(pi0_rep), nrow=length(pi0_rep), byrow=T))
names(pi0_rep_df) <- names(pi0_rep[[which(lengths(pi0_rep)>0)[1]]])
```



#### Problem 4.
Let's study how P-values and Q-values behave in a (very) discrete space.

Suppose you are given $p=1000$ coins and your task is to determine
which proportion of them are fair (that is, on average, 
will result in heads in 50% of tosses and in tails in 50% of tosses).
You do an experiment where you toss each coin 2 times and record
the number of heads $y_j \in \{0,1,2\}$ for each coin $j\leq p$.
Your null hypothesis for each coin is that the coin is fair.

(a) What is the null distribution of the outcome values of a single coin tossed 2 times?
What is the null distribution of (two-sided) 
P-values of a single coin tossed 2 times?
(In lectures, it was stated 
that the null distribution of P-values is Uniform(0,1), or equivalently, that
$\textrm{Pr}(P_j \leq t\,|\, \textrm{NULL}) = t$ for all $t \in [0,1]$, but 
now we learn that, 
in a discrete state space, we need to restrict this formula to exactly those
threshold values $t$ that correspond to the P-values attainable in the 
discrete state space.)

(b) Suppose that, with $p=1000$ coins, 
the observed counts of outcomes 0, 1 and 2 heads are 180, 366 and 454, respectively.
What is the observed P-value distribution of these $p$ observations?
How would you estimate $\pi_0$, the proportion of fair coins, 
by comparing the observed P-value distribution to the null distribution?
What is your estimate $\widehat{\pi}_0$?
What is your estimate of Q-value for obsevations whose P-value is 0.5?
(You may assume that all biased coins are fully biased, that is,
can only yield either heads or tails but never both.)

(c) What would be the estimate of $\widehat{\pi}_0(\lambda)$ from the lectures
HDS3 for value of $\lambda = 0.4, 0.5, 0.9$? Which of these three values (if any)
agrees with what you inferred in part (b)? 
Apply `qvalue()` to the set of your $p$ P-values and show 
`plot(qvalue())`. 
What is the estimated Q-value for obsevations whose P-value is 0.5 using `qvalue()`?
Does `qvalue()` seem to work well for these kinds of discrete data?



#### Problem 5.
Let's see how well $\textrm{lfdr}$ approximates the posterior probability of
the null hypothesis.
Simulate $p=1000$ P-values of which $p_0 = 800$ come  
from the null distribution (Uniform(0,1)) and
$m=200$ from the non-null distribution Beta(1,100).

(a)
For each P-value $P_j$, compute the posterior probability that the P-value comes from the 
null hypothesis ($H_j$) given the knowledge of the true non-null distribution 
and the true proportion $\pi_0 = p_0/p$.
(HINT: Expand 
$\textrm{Pr}(H_j\,|\,P_j, \pi_0, b_1=1, b_0=100)$ by using Bayes formula to switch
the roles of $H_j$ and $P_j$.)

(b)
Make a scatter plot of posteriors from part (a) and $\textrm{lfdr}$ 
values from `qvalue()` function applied to the P-values
using different colors according to the known null/non-null status.
Do these two quantities look similar?

(c)
Compute the average values separately for truly 
null and non-null hypotheses using (i) the exact posterior probability of 
null hypothesis and (ii) estimated $\textrm{lfdr}$.

## Junk Code
#req_var <- lapply(qval, `[`, c('qvalues', 'pi0'))
#req_var_df <- data.frame(matrix(unlist(req_var), ncol = max(lengths(req_var)), byrow = TRUE))
#names(req_var_df) <- names(req_var[[which(lengths(req_var)>0)[1]]])
#req_var_df_new<-as.data.frame(matrix(unlist(req_var), nrow=length(unlist(req_var[1]))))
#library(doFuture)
#registerDoFuture()
#plan(multiprocess)
#X <- 1:ncol(pval)
#qval <- foreach(x = X) %dopar% {qvalue(pval)}
#qval_func <- function(x) {qvalue(pval[,x])}
#qval <- sapply(pval_new, qval_func)
